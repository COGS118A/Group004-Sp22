{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFejFA_PdYWG"
      },
      "source": [
        "# COGS 118A- Project Checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-sgSEuxdYWP"
      },
      "source": [
        "# Names\n",
        "\n",
        "- Daniel Milton\n",
        "- Isabella Gonzalez\n",
        "- Dhruva Kolikineni\n",
        "- Harini Adivikolanu\n",
        "- Brandon Rocchio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ollXQi7qdYWQ"
      },
      "source": [
        "# Abstract \n",
        "Classification involves predicting discrete class labels for unlabeled data. given information on the data. The data we are working with is information from individuals' shopping trips at Walmart. The data is broken down into 7 observations, one of which is the trip type which tells us what type of shopping trip this customer was on, visit number which organizes the data into individual shopping trips, weekday that the trip was done on, UPC number of the item purchased, department of purchase and the fineline number which is a number that Walmart made helping us specify the items purchased. Given certain data such as what weekday it is, what department it was in, and what the UPC was, our goal is to be able to predict what type of shopping trip someone was on based off of a couple pieces of data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsSiF2NbdYWR"
      },
      "source": [
        "# Background\n",
        "\n",
        "Although supervised classification methods have been studied greatly throughout the years, research specific to this problem, on classifying a grocery trip based on the items bought remains low. There are multiple classification algorithms we can attempt in order to classify the 38 different types of shopping trips there are. K-nearest neighbors, decision trees, random forest classifiers, neural networks and logistic regression are commonly used in order to solve classification problems. Due to the low amount of research done in this specific problem area, this background section will be a small literature review of potential classification algorithms to use for our problem.\n",
        "\n",
        "Firstly, K Nearest Neighbors works based on the idea that for a target variable, the k number of patterns nearest to that target variable can provide useful information in order to properly classify the target variable. KNN assigns the target variable the classification of the majority of the nearest neighbors<a name=\"first\"></a>[<sup>[1]</sup>](#firstnote). The downfalls of KNN are that there is no right 'K' to choose and it can be computationally inefficient. Second, Decision Trees are another classification method we want to attempt, these are popular due to their good accuracy scores and their computational efficiency<a name=\"second\"></a>[<sup>[2]</sup>](#secondnote). The random forest classifier works by using multiple tree classifiers where each classifier is generated by using a random vector and each tree vottes for the most 'popular' class to classify an input vector<a name=\"third\"></a>[<sup>[3]</sup>](#thirdnote). This paper uses random forests to classify remote sensing which they concluded was just as accurate as using a support vector machine.\n",
        "\n",
        "Neural Networks and logistic regression are other potential algorithms we would like to try to classif our data with, there was no literature on any similar classification task to our project but a paper revealed that these two algorithms share common roots in statistical pattern recognition and that neural networks can be seen as a type of generalization from logistic regression<a name=\"fourth\"></a>[<sup>[4]</sup>](#fourthnote). Lastly, upon multiple attempts to find related work, Cui et al attempted this trip type classification using deep embedding logistic regression which incorporates logistic regression into a deep and narrow neural network<a name=\"fifth\"></a>[<sup>[5]</sup>](#fifthnote). We are hoping as we implement some of these algorithms to produce results and a discussion that can help future research for stores like Walmart to improve customers' shopping experiences or help understand/solve problems similar to this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ1qnjctdYWS"
      },
      "source": [
        "# Problem Statement\n",
        "\n",
        "Walmart currently employs a proprietary method to catogorize shopping trips into 38 distinct types. We have set out to create a clustering/catagorization model that, given a limited set of customer behavior features, predicts the shopping trip types.\n",
        "\n",
        "As an example for what these trip types may be: a customer may make a small daily dinner trip, a weekly large grocery trip, a trip to buy gifts for an upcoming holiday, or a seasonal trip to buy clothes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "qj6srIgKvVLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import *\n",
        "import io"
      ],
      "metadata": {
        "id": "l-iNutKkdjao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KShpUbledYWT"
      },
      "source": [
        "# Data\n",
        "\n",
        "UPDATED FROM PROPOSAL!\n",
        "\n",
        "You should have obtained and cleaned (if necessary) data you will use for this project.\n",
        "\n",
        "Please give the following infomration for each dataset you are using\n",
        "- link/reference to obtain it\n",
        "- description of the size of the dataset (# of variables, # of observations)\n",
        "- what an observation consists of\n",
        "- what some critical variables are, how they are represented\n",
        "- any special handling, transformations, cleaning, etc you have done should be demonstrated here!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Uploading train.csv\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "RK_ed0Hb0Oij",
        "outputId": "38c31798-ed4d-4a1b-ba69-310f52da6ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-12c4475d-0ab1-4c17-8c0a-d53c3b127bc5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-12c4475d-0ab1-4c17-8c0a-d53c3b127bc5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e8782929e7f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Uploading train.csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m   \"\"\"\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    121\u001b[0m   result = _output.eval_js(\n\u001b[1;32m    122\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m--> 123\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m    124\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Cannot read properties of undefined (reading '_uploadFiles')"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing:"
      ],
      "metadata": {
        "id": "peUEy9KryYal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cleaning the data:"
      ],
      "metadata": {
        "id": "KqehcILXEg-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Load in train dataset\n",
        "train = pd.read_csv(io.BytesIO(uploaded['train.csv']))\n",
        "\n",
        "## Drop Upc column\n",
        "train.drop(columns=['Upc'], inplace = True)\n",
        "\n",
        "\n",
        "## Find all rows with null values\n",
        "null_vals = train[train.isna().any(axis=1)]\n",
        "\n",
        "## I found from the above code that there is 4129 rows\n",
        "## with null values. From this I found that Department Descriptions of\n",
        "## PHARMACY RX always has a null FinelineNumber\n",
        "\n",
        "\n",
        "## A check of all unique values in these columns to make sure if any\n",
        "## further cleaning of the strings is needed\n",
        "department_unique = train['DepartmentDescription'].unique()\n",
        "weekday_unique = train['Weekday'].unique()\n",
        "scancount_unique = train['ScanCount'].unique()\n",
        "\n",
        "## Function to change the week days to quantitative variables, which allows us\n",
        "## to do analysis on the Weekday column\n",
        "def weekday_int_converter(x):\n",
        "  if x=='Monday':\n",
        "    return 0\n",
        "  elif x=='Tuesday':\n",
        "    return 1\n",
        "  elif x=='Wednesday':\n",
        "    return 2\n",
        "  elif x=='Thursday':\n",
        "    return 3\n",
        "  elif x=='Friday':\n",
        "    return 4\n",
        "  elif x=='Saturday':\n",
        "    return 5\n",
        "  elif x=='Sunday':\n",
        "    return 6 \n",
        "\n",
        "\n",
        "train['Weekday'] = train['Weekday'].apply(weekday_int_converter)\n",
        "\n",
        "## Filtered the train csv of all null values in the Department \n",
        "## Description column\n",
        "train = train[train['DepartmentDescription'].notna()]\n",
        "\n",
        "\n",
        "## I found there to be 60367 duplicate rows.\n",
        "duplicates = train[train.duplicated()]\n",
        "\n",
        "## Here I made a new df with all the duplicate rows dropped\n",
        "train_duplicates_dropped = train.drop_duplicates()\n",
        "\n",
        "\n",
        "## Might be useful to test model on both the original dataframe and the \n",
        "## dataframe with the duplicates dropped\n",
        "## Add a column indicated if the trip includes return\n",
        "train['Return'] = train['ScanCount']\n",
        "train\n",
        "def convert(x):\n",
        "    if x >= 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "train['Return'] = train['ScanCount'].apply(convert)\n",
        "train\n",
        "## Test any variable to see results\n",
        "train.head()"
      ],
      "metadata": {
        "id": "zAbrEZjvyb5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Test Set \n",
        "We will reserve 10% of our data for final validation. This will serve as our overall task performance benchmark."
      ],
      "metadata": {
        "id": "jvuoU8fEDAZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Aprl-kZQC-v5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Partitioning Train/Test Sets:\n",
        "Train/Test splitting will be implemented using Kfold. Since we are working with a very large dataset (500k+ datapoints), we will keep our K value small at K=5. This means each fold will have ~20% of our data in each fold. "
      ],
      "metadata": {
        "id": "QN-QjMRzC_KU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XvKWVo4DjNau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIINXk_pdYWU"
      },
      "source": [
        "# Proposed Solution\n",
        "\n",
        "Our proposed solution is to use train and fit a model to the traiing data and evaluate its perfromance on the testing data, all the whiel using cross validation to verify the accuracy of our results.\n",
        "\n",
        "The model we intend on using is still up for our team to decide, but given the nature of the problem of non-binary classification into 38 distinct catagories, we have isolated some models we beileve might be best suited for the task:\n",
        "\n",
        "- K- Nearest Neighbors\n",
        "- Decision tree classifiers\n",
        "- Neural Networks\n",
        "- Random Forest Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H06hurMDdYWX"
      },
      "source": [
        "# Preliminary results\n",
        "\n",
        "NEW SECTION!\n",
        "\n",
        "Please show any preliminary results you have managed to obtain.\n",
        "\n",
        "Examples would include:\n",
        "- Analyzing the suitability of a dataset or alogrithm for prediction/solving your problem \n",
        "- Performing feature selection or hand-designing features from the raw data. Describe the features available/created and/or show the code for selection/creation\n",
        "- Showing the performance of a base model/hyper-parameter setting.  Solve the task with one \"default\" algorithm and characterize the performance level of that base model.\n",
        "- Learning curves or validation curves for a particular model\n",
        "- Tables/graphs showing the performance of different models/hyper-parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EDA:"
      ],
      "metadata": {
        "id": "tZ-8pd_sBNLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before exploring the distributions of the data, we found that there were 38 different unique trip type categories in our dataset and plotted their frequency."
      ],
      "metadata": {
        "id": "wa1_44xW2PUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#taking first look at data\n",
        "train.TripType.unique()\n",
        "train['TripType'].value_counts()"
      ],
      "metadata": {
        "id": "7tH3uFI7BV-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting trip type frequency\n",
        "plt.figure(figsize = (12, 10))\n",
        "sns.set_style('darkgrid')\n",
        "sns.countplot(x = 'TripType', data = train, order = train['TripType'].value_counts().index).set(title='Different Trip Type Count')\n",
        "plt.xticks(rotation=90)\n",
        "sns.set(font_scale=2)"
      ],
      "metadata": {
        "id": "l1cfrRBS8kHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then did the same for department description and found that there were 68 different descriptions."
      ],
      "metadata": {
        "id": "nwCD6YNs2ZfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.DepartmentDescription.unique()\n",
        "train.DepartmentDescription.value_counts()"
      ],
      "metadata": {
        "id": "XIWan1572eGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To deduce whether there was a relationship between the department description and trip type, we plotted the frequency of the top 10 departments from which shoppers bought products from versus the trip type number. We found that trip types 40, 39, 37, 38, 7, and 8 seemed to be grocery trips of some sort, while trip types 36, 44, and 42 seemed to be household supply runs of some sort. Thus, it will be important to consider another factors like weekday vs. weekend, return vs. purchase, or the specific products purchased to differentiate trip types that seem to be overlapping like grocery/household trips."
      ],
      "metadata": {
        "id": "8G4LbRfV28qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TripType 40 seems to be a grocery trip type\n",
        "triptype_40 = train[train.TripType == 40].DepartmentDescription.value_counts().head(10).plot(kind = 'bar', title = 'Top Department Descriptions for TripType 40')"
      ],
      "metadata": {
        "id": "ZRU6Aq6629_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TripType 39 seems to be a grocery trip type/household trip\n",
        "triptype_39 = train[train.TripType == 39].DepartmentDescription.value_counts().head(10).plot(kind = 'bar', title = 'Top Department Descriptions for TripType 39')"
      ],
      "metadata": {
        "id": "NIgIYv6J3Cch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TripType 37 seems to be a grocery trip type - meat + frozen goods?\n",
        "triptype_37 = train[train.TripType == 37].DepartmentDescription.value_counts().head(10).plot(kind = 'bar', title = 'Top Department Descriptions for TripType 37')"
      ],
      "metadata": {
        "id": "PM40AMEn3Eli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TripType 38 seems to be a grocery trip type\n",
        "triptype_38 = train[train.TripType == 38].DepartmentDescription.value_counts().head(10).plot(kind = 'bar', title = 'Top Department Descriptions for TripType 38')"
      ],
      "metadata": {
        "id": "mjwuKoMK3JR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TripType 7 seems to be a grocery trip type\n",
        "triptype_7 = train[train.TripType == 7].DepartmentDescription.value_counts().head(10).plot(kind = 'bar', title = 'Top Department Descriptions for TripType 7')"
      ],
      "metadata": {
        "id": "z7tE7kTN3NCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TripType 8 seems to be a quick grocery run/errand run?\n",
        "triptype_8 = train[train.TripType == 8].DepartmentDescription.value_counts().head(10).plot(kind = 'bar', title = 'Top Department Descriptions for TripType 8')"
      ],
      "metadata": {
        "id": "Hl2VFzFb3RCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TripType 36 seems to be a household/personal care run\n",
        "triptype_36 = train[train.TripType == 36].DepartmentDescription.value_counts().head(10).plot(kind = 'bar', title = 'Top Department Descriptions for TripType 36')"
      ],
      "metadata": {
        "id": "NUEaDwcW3bjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TripType 44 seems to be a household run\n",
        "triptype_44 = train[train.TripType == 44].DepartmentDescription.value_counts().head(10).plot(kind = 'bar', title = 'Top Department Descriptions for TripType 44')"
      ],
      "metadata": {
        "id": "hRKqC8By3gc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TripType 42 seems to be a household run\n",
        "triptype_42 = train[train.TripType == 42].DepartmentDescription.value_counts().head(10).plot(kind = 'bar', title = 'Top Department Descriptions for TripType 42')"
      ],
      "metadata": {
        "id": "H6EPR3Nr3rWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting department description frequency\n",
        "## Creating a dictionary to convert department descriptions to ints\n",
        "department_list=list(train['DepartmentDescription'].unique())\n",
        "department_enumerate=list(enumerate(department_list))\n",
        "department_dict={v:k for k,v in department_enumerate}\n",
        "train.replace({'DepartmentDescription': department_dict},inplace=True)\n",
        "\n",
        "\n",
        "## Plotting\n",
        "plt.figure(figsize = (12, 10))\n",
        "sns.set_style('darkgrid')\n",
        "sns.countplot(x = 'DepartmentDescription', data = train, order = train['DepartmentDescription'].value_counts().index).set(title='Deparment Description Frequency')\n",
        "plt.xticks(fontsize=7)"
      ],
      "metadata": {
        "id": "cCAm2hpmBKhZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "d82b8978-5317-4c24-91e7-ad53a4a87faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-eb2f158a0403>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#plotting department description frequency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m## Creating a dictionary to convert department descriptions to ints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdepartment_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DepartmentDescription'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdepartment_enumerate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepartment_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdepartment_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdepartment_enumerate\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To start exploring these other factors, we analyzed the frequency of trips made on each weekday and created a pivot table to conduct a bivariate analysis between TripType and Weekday. This allowed us to visualize the probability that a certain TripType falls on a certain day. Interestingly, we found that TripType frequency seemed similar throughot the week, regardless of the weekday."
      ],
      "metadata": {
        "id": "IYXtZfVk4A1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting weekday frequency\n",
        "plt.figure(figsize = (12, 10))\n",
        "sns.set_style('darkgrid')\n",
        "sns.countplot(x = 'Weekday', data = train, order = train['Weekday'].value_counts().index).set(title='Weekday Frequency')\n",
        "plt.xticks(rotation=90)"
      ],
      "metadata": {
        "id": "RsUAE0dF9Bpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating a pivot table to do bivariate analysis between TripType and\n",
        "## Weekday\n",
        "Weekday_TripType = (\n",
        "   train\n",
        "    .pivot_table(index='TripType', columns='Weekday', values=None, aggfunc='size').fillna(0)\n",
        ")\n",
        "\n",
        "\n",
        "## calculating all the probabilities of getting a certain trip type,\n",
        "## depending on the day of the week\n",
        "for i in Weekday_TripType:\n",
        "    Weekday_TripType[i] = Weekday_TripType[i] / Weekday_TripType[i].sum()\n",
        "    \n",
        "## Here I removed trip type 999 because it skewed our graph\n",
        "Weekday_TripType = Weekday_TripType[:-1]\n",
        "Weekday_TripType"
      ],
      "metadata": {
        "id": "sGYdmeQcATJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Here I used a for loop to plot the \n",
        "## Probabilies depending on the trip type\n",
        "## for every day of the week. Here we can \n",
        "## see that trip types remain very similiar across\n",
        "## the week days. This might be useful to consider when we decide how\n",
        "## much weight to give weekdays in our model.\n",
        "weekdays = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday',}\n",
        "for i in Weekday_TripType:\n",
        "    plt.figure()\n",
        "    plt.plot(Weekday_TripType.index, Weekday_TripType[i])\n",
        "    plt.title(weekdays[i] + ' Probability Distribution')"
      ],
      "metadata": {
        "id": "ZXn29YqlAVPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category_counter = train.copy()\n",
        "category_counter.fillna(0, inplace = True)\n",
        "department_list=list(train['DepartmentDescription'].unique())\n",
        "department_enumerate=list(enumerate(department_list))\n",
        "department_dict={v:k for k,v in department_enumerate}\n",
        "df = pd.DataFrame(index = train['VisitNumber'], columns = train['DepartmentDescription'].unique())\n",
        "df.fillna(0, inplace = True)\n",
        "def converter(x):\n",
        "    return str(x) + ','\n",
        "def converter2(x):\n",
        "    return x.split(',')[:-1]\n",
        "category_counter['DepartmentDescription'] = category_counter['DepartmentDescription'].apply(converter)\n",
        "lists = category_counter.groupby('VisitNumber')['DepartmentDescription'].sum().apply(converter2)\n",
        "df = df.drop(df.columns[9], axis = 1)\n",
        "for i in lists.index[:500]:\n",
        "    for j in lists[i]:\n",
        "        df.loc[i, j] + 1\n",
        "mytestgrouped_categories_norm = (df - df.mean()) / (df.max() - df.min())"
      ],
      "metadata": {
        "id": "ep9QGM7zTkoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a4_dims = (13, 9)\n",
        "fig, ax = plt.subplots(figsize=a4_dims)\n",
        "sns.heatmap(ax=ax, data=mytestgrouped_categories_norm.T, linecolor='lightgrey', linewidths=.00000000000000001)\n",
        "#heatmap = ax.pcolor(mytestgrouped_categories_norm, cmap=plt.cm.Blues, alpha=0.8)\n",
        "#ax.set_xticklabels(mytestgrouped_categories.columns, minor=False)\n",
        "#ax.set_yticklabels(mytestgrouped_categories_norm.index, minor=False)\n",
        "#ax.invert_yaxis()\n",
        "ax.xaxis.tick_top()\n",
        "#ax.set_yticks(np.arange(mytestgrouped_categories_norm.shape[1]) + 1, minor=False)\n",
        "#ax.set_xticks(np.arange(mytestgrouped_categories_norm.shape[0]) + 1, minor=False)\n",
        "#plt.xticks(rotation=90)\n",
        "#plt.rc('xtick', labelsize=10)\n",
        "plt.title('TripType',y=1.04)"
      ],
      "metadata": {
        "id": "Yh2vV8sETvls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Model \n",
        "for a start to our analysis, we are going to take a look at what we beleive to be a \"base case\" using a Logistic Regression Classifer with a somewhat arbritary l2 regularization. This gives us a metric that we can utilize more exhastive methods to try and beat"
      ],
      "metadata": {
        "id": "3zToTOekyHee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = pd.get_dummies(train, columns=['DepartmentDescription'])\n",
        "temp  = temp.dropna()\n",
        "temp.isnull().values.any()\n",
        "temp"
      ],
      "metadata": {
        "id": "eVoQCdKQVI7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = temp\n",
        "X = X.drop(columns=['TripType'])\n",
        "y = temp['TripType']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size = .75)"
      ],
      "metadata": {
        "id": "e3OM2cpKHrx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Logistic Regression and standard scaler pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
        "pipe.fit(X_train, y_train)  # apply scaling on training data\n",
        "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
        "                ('logisticregression', LogisticRegression())])\n",
        "pipe.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "ZLssi3qVViZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYcrqzSBdYWW"
      },
      "source": [
        "# Evaluation Metrics\n",
        "\n",
        "(we need to decide on our multiclass evaluation metric, inital thoughts make me belive we should do f1)Our evaluation metric will involve using a multi-class logarithmic loss fuction. For each row, we will find a set of predicted probabilities for every trip type and apply the logarithmic loss formula. As we optimize our model, we hope to minimize this function as a metric of how well our model is performing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Selection\n",
        "In pursuit of selecting only relevent features for our analysis, we decided to use... (Dimentionality reductuion blah blah PCA analysis)"
      ],
      "metadata": {
        "id": "s6fkSrDsxYwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "hFBguUkrw7Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our PCA anaysis revealed the following relevent features:\n",
        " *"
      ],
      "metadata": {
        "id": "7Xv0ywg3CenU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qS-36pe3JLSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Training\n",
        "In search of the optimal model, we will instantiate each of our hypothesized algorithms and perform a hyperparameter sweep on each to select the best parameters. This allows each classifier to put it's best foot forward during their eventual model comparison."
      ],
      "metadata": {
        "id": "Uf-_hZY6JLvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pipelineing:"
      ],
      "metadata": {
        "id": "YCw0di3Guecz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "knn_pipe = make_pipeline(steps=[(\"standard\",StandardScaler),(\"KNN\")])\n",
        "dt_pipe = make_pipeline(steps=[(\"standard\",StandardScaler)])\n",
        "NN_pipe = make_pipeline(steps=[(\"standard\",StandardScaler)])\n",
        "Rforest = make_pipeline(steps=[(\"standard\", StandardScaler])\n",
        "\n"
      ],
      "metadata": {
        "id": "qvbfQwYauxRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##K-Nearest Neighbors\n"
      ],
      "metadata": {
        "id": "5qJ5KpQLujXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nl6aU1Xsu4YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decision Tree"
      ],
      "metadata": {
        "id": "yISEWSGJu5G9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Fb3BSFVNvAkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Neural Network"
      ],
      "metadata": {
        "id": "LmDYYDtzvAys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MX0UuUPBvNw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "7QZa0xv0vN5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZWthBwZBvSk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM Classifier??"
      ],
      "metadata": {
        "id": "6apqmvvHvTWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "e8YUYFVuvdVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Comparison"
      ],
      "metadata": {
        "id": "JjJ0PxoHvlkY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u0dK9BUdYWY"
      },
      "source": [
        "# Ethics & Privacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp52ycowdYWZ"
      },
      "source": [
        "Our data does not involve anyones data or identity so it would be difficult to find a breach of ethics or privacy. One ethical dilemma that might arise is the increasing amount of data available to big corporations and how they’re using this big data to fine tune their products to keep the average person consuming even more. It would be good to question whether it is completely ethical for corporations to treat everyone as a number to maximize their profits.\n",
        "\n",
        "Another note on privacy is the trip type classification labels. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhplYiLCdYWZ"
      },
      "source": [
        "# Team Expectations "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dqZncRZdYWa"
      },
      "source": [
        "Put things here that cement how you will interact/communicate as a team, how you will handle conflict and difficulty, how you will handle making decisions and setting goals/schedule, how much work you expect from each other, how you will handle deadlines, etc...\n",
        "\n",
        "- Arrange bi-weekly meetings that works with everyones schedule\n",
        "- Use a discord server to communicate with one another\n",
        "- Make use of project managment software to track progress\n",
        "- Be mindful of git -pull-push-overwrites such that no code is overwritten or needlessly repeated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ndy_9D4UdYWa"
      },
      "source": [
        "# Project Timeline Proposal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYsKpfridYWb"
      },
      "source": [
        "UPDATE THE PROPOSAL TIMELINE ACCORDING TO WHAT HAS ACTUALLY HAPPENED AND HOW IT HAS EFFECTED YOUR FUTURE PLANS\n",
        "\n",
        "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
        "|---|---|---|---|\n",
        "| 4/24  |  3:30 PM |  Edit, finalize, and submit proposal; Search for datasets  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part | \n",
        "| 5/16  |  9 PM |  Delegate tasks for first checkpoint and discuss wrangling, cleaning, and EDA plan | Import and wrangle data, do some EDA | \n",
        "| 5/19  | 9 PM  | Edit and finalize data cleaning and wrangling/EDA  | Review/discuss EDA, debug, and submit checkpoint   |\n",
        "| 5/23  | 7 PM  | Finalize project/conclusion/discussion | Discuss conclusion   |\n",
        "| 6/8  | Before 11:59 PM  | NA | Turn in Final Project  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ0o-4_PdYWc"
      },
      "source": [
        "# Footnotes\n",
        "<a name=\"first\"></a>1.[^](#firstnote): Oliver Kramer. K Nearest Neighbors. https://link.springer.com/chapter/10.1007/978-3-642-38652-7_2<br> \n",
        "<a name=\"second\"></a>2.[^](#secondnote): Srivastava et al. (1999) Parallel Forumlations of Decision Tree Classfication Algorithms. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.41.7475&rep=rep1&type=pdf<br>\n",
        "<a name=\"third\"></a>3.[^](#thirdnote): M. Pal (2005).Random forest classifier for remote sensing classification. https://www.tandfonline.com/doi/pdf/10.1080/01431160412331269698?casa_token=e78vG4sBDLcAAAAA:p9nt0mSjEMuazyQsDjprmwIIFt9aNRk9EtF7eKRyNozF6FsAskuvXKrMxnnftOK0xFjlUm5MX9g.<br>\n",
        "<a name=\"fourth\"></a>4.[^](#fourthnote): Stephan Dreiseitl and Lucila Ohno_Machado. (2002). Logistic regression and artificial neural network classification models: a methodology review. https://www.sciencedirect.com/science/article/pii/S1532046403000340.<br>\n",
        "<a name=\"fifth\"></a>5.[^](#fifthnote): Cui et al. (2018). Deep Embedding Logistic Regression. https://ieeexplore.ieee.org/document/8588790\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1rVn2mEdYWd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Checkpoint_group004.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}