{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/COGS118A/Group004-Sp22/blob/main/Final_Project_group004.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFejFA_PdYWG"
      },
      "source": [
        "# COGS 118A- Project Checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-sgSEuxdYWP"
      },
      "source": [
        "# Names\n",
        "\n",
        "- Daniel Milton\n",
        "- Isabella Gonzalez\n",
        "- Dhruva Kolikineni\n",
        "- Harini Adivikolanu\n",
        "- Brandon Rocchio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ollXQi7qdYWQ"
      },
      "source": [
        "# Abstract \n",
        "Classification involves predicting discrete class labels for unlabeled data. given information on the data. The data we are working with is information from individuals' shopping trips at Walmart. The data is broken down into 7 observations, one of which is the trip type which tells us what type of shopping trip this customer was on, visit number which organizes the data into individual shopping trips, weekday that the trip was done on, UPC number of the item purchased, department of purchase and the fineline number which is a number that Walmart made helping us specify the items purchased. Given certain data such as what weekday it is, what department it was in, and what the UPC was, our goal is to be able to predict what type of shopping trip someone was on based off of a couple pieces of data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsSiF2NbdYWR"
      },
      "source": [
        "# Background\n",
        "\n",
        "Although supervised classification methods have been studied greatly throughout the years, research specific to this problem, on classifying a grocery trip based on the items bought remains low. There are multiple classification algorithms we can attempt in order to classify the 38 different types of shopping trips there are. K-nearest neighbors, decision trees, random forest classifiers, neural networks and logistic regression are commonly used in order to solve classification problems. Due to the low amount of research done in this specific problem area, this background section will be a small literature review of potential classification algorithms to use for our problem.\n",
        "\n",
        "Firstly, K Nearest Neighbors works based on the idea that for a target variable, the k number of patterns nearest to that target variable can provide useful information in order to properly classify the target variable. KNN assigns the target variable the classification of the majority of the nearest neighbors<a name=\"first\"></a>[<sup>[1]</sup>](#firstnote). The downfalls of KNN are that there is no right 'K' to choose and it can be computationally inefficient. Second, Decision Trees are another classification method we want to attempt, these are popular due to their good accuracy scores and their computational efficiency<a name=\"second\"></a>[<sup>[2]</sup>](#secondnote). The random forest classifier works by using multiple tree classifiers where each classifier is generated by using a random vector and each tree vottes for the most 'popular' class to classify an input vector<a name=\"third\"></a>[<sup>[3]</sup>](#thirdnote). This paper uses random forests to classify remote sensing which they concluded was just as accurate as using a support vector machine.\n",
        "\n",
        "Neural Networks and logistic regression are other potential algorithms we would like to try to classif our data with, there was no literature on any similar classification task to our project but a paper revealed that these two algorithms share common roots in statistical pattern recognition and that neural networks can be seen as a type of generalization from logistic regression<a name=\"fourth\"></a>[<sup>[4]</sup>](#fourthnote). Lastly, upon multiple attempts to find related work, Cui et al attempted this trip type classification using deep embedding logistic regression which incorporates logistic regression into a deep and narrow neural network<a name=\"fifth\"></a>[<sup>[5]</sup>](#fifthnote). We are hoping as we implement some of these algorithms to produce results and a discussion that can help future research for stores like Walmart to improve customers' shopping experiences or help understand/solve problems similar to this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ1qnjctdYWS"
      },
      "source": [
        "# Problem Statement\n",
        "\n",
        "Walmart currently employs a proprietary method to catogorize shopping trips into 38 distinct types. We have set out to create a clustering/catagorization model that, given a limited set of customer behavior features, predicts the shopping trip types.\n",
        "\n",
        "As an example for what these trip types may be: a customer may make a small daily dinner trip, a weekly large grocery trip, a trip to buy gifts for an upcoming holiday, or a seasonal trip to buy clothes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj6srIgKvVLV"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l-iNutKkdjao"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import *\n",
        "import io\n",
        "import tensorflow as tp\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KShpUbledYWT"
      },
      "source": [
        "# Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peUEy9KryYal"
      },
      "source": [
        "## Data Preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAbrEZjvyb5F",
        "outputId": "511eb82e-a78b-4f71-b675-2b96b1f54e18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   TripType  VisitNumber  Weekday           Upc  ScanCount  \\\n",
            "0       999            5        4  6.811315e+10         -1   \n",
            "1        30            7        4  6.053882e+10          1   \n",
            "2        30            7        4  7.410811e+09          1   \n",
            "3        26            8        4  2.238404e+09          2   \n",
            "4        26            8        4  2.006614e+09          2   \n",
            "\n",
            "   DepartmentDescription  FinelineNumber  Return  \n",
            "0     FINANCIAL SERVICES          1000.0       1  \n",
            "1                  SHOES          8931.0       0  \n",
            "2          PERSONAL CARE          4504.0       0  \n",
            "3  PAINT AND ACCESSORIES          3565.0       0  \n",
            "4  PAINT AND ACCESSORIES          1017.0       0  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(642925, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "## Load in train dataset\n",
        "train = pd.read_csv((r'/content/train.csv'))\n",
        "\n",
        "## Drop Upc column\n",
        "#train.drop(columns=['Upc'], inplace = True)\n",
        "\n",
        "\n",
        "## Find all rows with null values\n",
        "null_vals = train[train.isna().any(axis=1)]\n",
        "\n",
        "## I found from the above code that there is 4129 rows\n",
        "## with null values. From this I found that Department Descriptions of\n",
        "## PHARMACY RX always has a null FinelineNumber\n",
        "\n",
        "\n",
        "## A check of all unique values in these columns to make sure if any\n",
        "## further cleaning of the strings is needed\n",
        "department_unique = train['DepartmentDescription'].unique()\n",
        "weekday_unique = train['Weekday'].unique()\n",
        "scancount_unique = train['ScanCount'].unique()\n",
        "\n",
        "## Function to change the week days to quantitative variables, which allows us\n",
        "## to do analysis on the Weekday column\n",
        "def weekday_int_converter(x):\n",
        "  if x=='Monday':\n",
        "    return 0\n",
        "  elif x=='Tuesday':\n",
        "    return 1\n",
        "  elif x=='Wednesday':\n",
        "    return 2\n",
        "  elif x=='Thursday':\n",
        "    return 3\n",
        "  elif x=='Friday':\n",
        "    return 4\n",
        "  elif x=='Saturday':\n",
        "    return 5\n",
        "  elif x=='Sunday':\n",
        "    return 6 \n",
        "\n",
        "\n",
        "train['Weekday'] = train['Weekday'].apply(weekday_int_converter)\n",
        "\n",
        "## Filtered the train csv of all null values in the Department \n",
        "## Description column\n",
        "train = train[train['DepartmentDescription'].notna()]\n",
        "\n",
        "\n",
        "## We found there to be 60367 duplicate rows.\n",
        "duplicates = train[train.duplicated()]\n",
        "\n",
        "## We decided not to drop the duplicates as they may provide valuable data.\n",
        "#train_duplicates_dropped = train.drop_duplicates()\n",
        "\n",
        "\n",
        "## Add a column indicated if the trip includes return\n",
        "train['Return'] = train['ScanCount']\n",
        "train\n",
        "def convert(x):\n",
        "    if x >= 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "train['Return'] = train['ScanCount'].apply(convert)\n",
        "\n",
        "#Dropping NA values\n",
        "train = train.dropna()\n",
        "\n",
        "## Test any variable to see results\n",
        "train.shape\n",
        "print(train.head())\n",
        "train.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgTY9ILkIYIG"
      },
      "source": [
        "In order to clean our data, we imported the training data set, dropped the UPC column since we determined it was not a significant variable, changed weekdays into numerical categories, dropped all NA values as well as duplicate rows. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMw1wAngC359"
      },
      "source": [
        "##Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eVoQCdKQVI7t"
      },
      "outputs": [],
      "source": [
        "#temp = pd.get_dummies(train, columns=['DepartmentDescription'])\n",
        "#temp  = temp.dropna()\n",
        "#temp.isnull().values.any()\n",
        "#temp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Feature Engineering\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Create a new df to append a groupby series of the sum of items bought in each trip\n",
        "items_bought_each_trip = pd.DataFrame()\n",
        "\n",
        "\n",
        "\n",
        "## Groupby function asssigned to new col\n",
        "items_bought_each_trip['item_sum'] = train.groupby(['VisitNumber'])['ScanCount'].sum()\n",
        "\n",
        "\n",
        "## This merge creates a df that makes only one row for each trip containing its sum of items\n",
        "merged = items_bought_each_trip.reset_index().merge(train[['TripType', 'VisitNumber', 'Weekday']].drop_duplicates(), on='VisitNumber', how = 'left')\n",
        "\n",
        "\n",
        "\n",
        "## Another dataframe to OHE departments\n",
        "new_df = train.groupby(['VisitNumber', 'DepartmentDescription'])['ScanCount'].sum().reset_index()\n",
        "\n",
        "## OHE\n",
        "temp = pd.get_dummies(new_df, columns=['DepartmentDescription'])\n",
        "\n",
        "## THis bit of code I iterated throught the department desciption columns and made an\n",
        "## aggregate function to make only one row per trip with each deparment description it \n",
        "## had showing as one\n",
        "new_dict = {}\n",
        "for i in temp:\n",
        "    new_dict[i] = 'sum'\n",
        "final = temp.groupby(temp['VisitNumber']).aggregate(new_dict)\n",
        "final = final.iloc[: , 1:].reset_index()\n",
        "\n",
        "\n",
        "\n",
        "## Merging the final df and merged df\n",
        "temp = merged.merge(final, on='VisitNumber')\n"
      ],
      "metadata": {
        "id": "ghk3CZ3-7ml6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvuoU8fEDAZG"
      },
      "source": [
        "## Final Test Set \n",
        "We will reserve 10% of our data for final validation. This will serve as our overall task performance benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Aprl-kZQC-v5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "088982b5-bc91-4c96-f136-168ccb0d23d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['VisitNumber', 'item_sum', 'Weekday', 'ScanCount',\n",
            "       'DepartmentDescription_1-HR PHOTO', 'DepartmentDescription_ACCESSORIES',\n",
            "       'DepartmentDescription_AUTOMOTIVE', 'DepartmentDescription_BAKERY',\n",
            "       'DepartmentDescription_BATH AND SHOWER', 'DepartmentDescription_BEAUTY',\n",
            "       'DepartmentDescription_BEDDING',\n",
            "       'DepartmentDescription_BOOKS AND MAGAZINES',\n",
            "       'DepartmentDescription_BOYS WEAR',\n",
            "       'DepartmentDescription_BRAS & SHAPEWEAR',\n",
            "       'DepartmentDescription_CAMERAS AND SUPPLIES',\n",
            "       'DepartmentDescription_CANDY, TOBACCO, COOKIES',\n",
            "       'DepartmentDescription_CELEBRATION', 'DepartmentDescription_COMM BREAD',\n",
            "       'DepartmentDescription_CONCEPT STORES',\n",
            "       'DepartmentDescription_COOK AND DINE', 'DepartmentDescription_DAIRY',\n",
            "       'DepartmentDescription_DSD GROCERY',\n",
            "       'DepartmentDescription_ELECTRONICS',\n",
            "       'DepartmentDescription_FABRICS AND CRAFTS',\n",
            "       'DepartmentDescription_FINANCIAL SERVICES',\n",
            "       'DepartmentDescription_FROZEN FOODS', 'DepartmentDescription_FURNITURE',\n",
            "       'DepartmentDescription_GIRLS WEAR, 4-6X  AND 7-14',\n",
            "       'DepartmentDescription_GROCERY DRY GOODS',\n",
            "       'DepartmentDescription_HARDWARE',\n",
            "       'DepartmentDescription_HEALTH AND BEAUTY AIDS',\n",
            "       'DepartmentDescription_HOME DECOR',\n",
            "       'DepartmentDescription_HOME MANAGEMENT',\n",
            "       'DepartmentDescription_HORTICULTURE AND ACCESS',\n",
            "       'DepartmentDescription_HOUSEHOLD CHEMICALS/SUPP',\n",
            "       'DepartmentDescription_HOUSEHOLD PAPER GOODS',\n",
            "       'DepartmentDescription_IMPULSE MERCHANDISE',\n",
            "       'DepartmentDescription_INFANT APPAREL',\n",
            "       'DepartmentDescription_INFANT CONSUMABLE HARDLINES',\n",
            "       'DepartmentDescription_JEWELRY AND SUNGLASSES',\n",
            "       'DepartmentDescription_LADIES SOCKS',\n",
            "       'DepartmentDescription_LADIESWEAR',\n",
            "       'DepartmentDescription_LARGE HOUSEHOLD GOODS',\n",
            "       'DepartmentDescription_LAWN AND GARDEN',\n",
            "       'DepartmentDescription_LIQUOR,WINE,BEER',\n",
            "       'DepartmentDescription_MEAT - FRESH & FROZEN',\n",
            "       'DepartmentDescription_MEDIA AND GAMING',\n",
            "       'DepartmentDescription_MENS WEAR', 'DepartmentDescription_MENSWEAR',\n",
            "       'DepartmentDescription_OFFICE SUPPLIES',\n",
            "       'DepartmentDescription_OPTICAL - FRAMES',\n",
            "       'DepartmentDescription_OPTICAL - LENSES',\n",
            "       'DepartmentDescription_OTHER DEPARTMENTS',\n",
            "       'DepartmentDescription_PAINT AND ACCESSORIES',\n",
            "       'DepartmentDescription_PERSONAL CARE',\n",
            "       'DepartmentDescription_PETS AND SUPPLIES',\n",
            "       'DepartmentDescription_PHARMACY OTC',\n",
            "       'DepartmentDescription_PHARMACY RX',\n",
            "       'DepartmentDescription_PLAYERS AND ELECTRONICS',\n",
            "       'DepartmentDescription_PLUS AND MATERNITY',\n",
            "       'DepartmentDescription_PRE PACKED DELI',\n",
            "       'DepartmentDescription_PRODUCE', 'DepartmentDescription_SEAFOOD',\n",
            "       'DepartmentDescription_SEASONAL', 'DepartmentDescription_SERVICE DELI',\n",
            "       'DepartmentDescription_SHEER HOSIERY', 'DepartmentDescription_SHOES',\n",
            "       'DepartmentDescription_SLEEPWEAR/FOUNDATIONS',\n",
            "       'DepartmentDescription_SPORTING GOODS',\n",
            "       'DepartmentDescription_SWIMWEAR/OUTERWEAR',\n",
            "       'DepartmentDescription_TOYS', 'DepartmentDescription_WIRELESS'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_set, test_set = train_test_split(temp, test_size = 0.1)\n",
        "final_y_test = test_set['TripType']\n",
        "final_X_test = test_set.drop(columns=[\"TripType\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DTghtUQONxMh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN-QjMRzC_KU"
      },
      "source": [
        "##Partitioning Train/Test Sets:\n",
        "Initially, we decided that our train/test data splitting will be implemented using Kfold CV. But since we are working very large dataset (500k+ datapoints), we found we had insufficent compute resource to run too many iterations. Now, the data splitting will be done through a simple train-test partition with 70% of the datapoints in the training set and 30% in the validation set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "e3OM2cpKHrx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49a1e208-22f2-444e-ab6b-5d500dea81bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(38,)\n",
            "(60174, 72)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#X = temp\n",
        "y = train_set['TripType']\n",
        "X = train_set.drop(columns=['TripType'])\n",
        "#y = temp['TripType']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size = .70, shuffle=True)\n",
        "print (y_test.unique().shape)\n",
        "print(X_train.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIINXk_pdYWU"
      },
      "source": [
        "# Proposed Solution\n",
        "\n",
        "Our proposed solution is to use train and fit a model to the training data, evaluate its performance on the testing data, and then use cross validation to verify the accuracy of our results.\n",
        "\n",
        "Given the nature of our problem, non-binary classification of trip type into 38 distinct categories, we have isolated some models we believe would be best suited for the task:\n",
        "\n",
        "- SVM\n",
        "- Random Forest \n",
        "- Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKNmuxrbKzkd"
      },
      "source": [
        "####Note On Model Selection:\n",
        "The nature of our dataset naturally lends itself to certain models and leave other ones unviable. Because of the number of features we have and the large dataset, K-NN Classifiers would suffer. Logistic regression is another such classifer that we can rule out due to the non binary catagorization of our dataset. \n",
        "\n",
        "we also are constrained in our compute time and resource, this means that certain classifiers that have more complex training times must be evaluated more harshly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYcrqzSBdYWW"
      },
      "source": [
        "# Evaluation Metrics\n",
        "\n",
        "Given the context of our data and intended solution, we need a multiclass evaluation metric. Thus, we've chosen to evaluate the performance of our benchmark model and solution models by calculating the F1-score. The F1-score combines two metrics, precision and recall, using their harmonic mean to give one single number. Therefore, the F1-score would allow us to take both the number of false positives and false negatives of trip type classification into account. Further, the F1-score is a useful evaluation metric to compare against the accuracy scores of our proposed models, considering that we have an uneven class distribution. For this reason, we chose to calculate the F1-score with a weighted average to take into account the proportion of each trip type label in our dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf-_hZY6JLvZ"
      },
      "source": [
        "#Model Training\n",
        "In search of the optimal model, we will instantiate each of our hypothesized algorithms and perform a hyperparameter sweep on each to select the best parameters. This allows each classifier to put it's best foot forward during their eventual model comparison.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N0PN0AqONQJ"
      },
      "source": [
        "\n",
        "#####*Note on Constrained Model Training:\n",
        "Given the compute resource contraint, we will be training all of our models with a class balanced random sample (75k samples) of the larger training dataset. This is to allow for faster training times, allowing more granular hyperparameter tuning. We will then train/test the best model on the entire dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWeiAuXVs2aj"
      },
      "source": [
        "## Random Forests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "1fGRh7JpssCF"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "#using random forest to classify data with dimensionality reduced to first feature\n",
        "rfc = RandomForestClassifier(max_depth=30)\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "pred = rfc.predict(X_test)\n",
        "#pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "hfEpdjCfssP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d68a0d8-1469-49b2-ae68-390e3aa904d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6386583947266382"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "rfc.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3sOfiM4-Aq25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qUO1fYn6s8-q",
        "outputId": "32c16d6a-e3c1-4d2c-ba48-876a4a4dd3a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           3       0.81      0.93      0.86      1104\n",
            "           4       0.05      0.01      0.02       107\n",
            "           5       0.69      0.78      0.73      1453\n",
            "           6       0.70      0.61      0.65       376\n",
            "           7       0.64      0.61      0.62      1687\n",
            "           8       0.75      0.85      0.79      3639\n",
            "           9       0.65      0.82      0.73      2902\n",
            "          12       0.67      0.02      0.04        92\n",
            "          15       0.58      0.26      0.36       315\n",
            "          18       0.36      0.23      0.28       175\n",
            "          19       0.31      0.07      0.12       109\n",
            "          20       0.52      0.46      0.49       198\n",
            "          21       0.48      0.46      0.47       185\n",
            "          22       0.45      0.08      0.14       277\n",
            "          23       0.00      0.00      0.00        43\n",
            "          24       0.51      0.42      0.46       758\n",
            "          25       0.56      0.66      0.61      1117\n",
            "          26       0.43      0.10      0.16       155\n",
            "          27       0.55      0.42      0.48       235\n",
            "          28       0.42      0.22      0.29       138\n",
            "          29       0.25      0.04      0.06       134\n",
            "          30       0.53      0.18      0.27       347\n",
            "          31       0.69      0.60      0.64       178\n",
            "          32       0.60      0.70      0.65       584\n",
            "          33       0.49      0.43      0.46       384\n",
            "          34       0.60      0.45      0.51       237\n",
            "          35       0.46      0.45      0.45       623\n",
            "          36       0.45      0.57      0.51       939\n",
            "          37       0.41      0.23      0.30       836\n",
            "          38       0.45      0.30      0.36       859\n",
            "          39       0.47      0.69      0.56      2940\n",
            "          40       0.70      0.92      0.79      1829\n",
            "          41       0.18      0.01      0.02       162\n",
            "          42       0.37      0.14      0.20       528\n",
            "          43       0.16      0.01      0.02       282\n",
            "          44       0.32      0.02      0.05       365\n",
            "         999       0.97      0.75      0.85      2363\n",
            "\n",
            "    accuracy                           0.63     28655\n",
            "   macro avg       0.49      0.39      0.41     28655\n",
            "weighted avg       0.61      0.63      0.61     28655\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn. metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhQOgNxLs9Ly"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "print(\"Overall Accuracy:\",accuracy_score(y_test, pred))\n",
        "print(\"Overall Precision:\",precision_score(y_test, pred, average='macro'))\n",
        "print(\"Overall Recall:\",recall_score(y_test, pred, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpC1eDt1s9R8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix = confusion_matrix(y_test, pred)\n",
        "\n",
        "print(confusion_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUqO4OSUHvPk"
      },
      "source": [
        "###Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fm9rqUMLHvXq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8950759b-3f8f-483a-fa5f-5e5ab3965ea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 264, in _score\n",
            "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1131, in f1_score\n",
            "    zero_division=zero_division,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1270, in fbeta_score\n",
            "    zero_division=zero_division,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1544, in precision_recall_fscore_support\n",
            "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1367, in _check_set_wise_labels\n",
            "    \"choose another average setting, one of %r.\" % (y_type, average_options)\n",
            "ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5] END ....................n_estimators=100;, score=nan total time=  10.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 264, in _score\n",
            "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1131, in f1_score\n",
            "    zero_division=zero_division,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1270, in fbeta_score\n",
            "    zero_division=zero_division,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1544, in precision_recall_fscore_support\n",
            "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1367, in _check_set_wise_labels\n",
            "    \"choose another average setting, one of %r.\" % (y_type, average_options)\n",
            "ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5] END ....................n_estimators=100;, score=nan total time=   9.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 264, in _score\n",
            "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1131, in f1_score\n",
            "    zero_division=zero_division,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1270, in fbeta_score\n",
            "    zero_division=zero_division,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1544, in precision_recall_fscore_support\n",
            "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1367, in _check_set_wise_labels\n",
            "    \"choose another average setting, one of %r.\" % (y_type, average_options)\n",
            "ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5] END ....................n_estimators=100;, score=nan total time=   8.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 264, in _score\n",
            "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1131, in f1_score\n",
            "    zero_division=zero_division,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1270, in fbeta_score\n",
            "    zero_division=zero_division,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1544, in precision_recall_fscore_support\n",
            "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1367, in _check_set_wise_labels\n",
            "    \"choose another average setting, one of %r.\" % (y_type, average_options)\n",
            "ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5] END ....................n_estimators=100;, score=nan total time=   8.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 264, in _score\n",
            "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1131, in f1_score\n",
            "    zero_division=zero_division,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1270, in fbeta_score\n",
            "    zero_division=zero_division,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1544, in precision_recall_fscore_support\n",
            "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1367, in _check_set_wise_labels\n",
            "    \"choose another average setting, one of %r.\" % (y_type, average_options)\n",
            "ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5] END ....................n_estimators=100;, score=nan total time=   9.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 264, in _score\n",
            "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1131, in f1_score\n",
            "    zero_division=zero_division,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1270, in fbeta_score\n",
            "    zero_division=zero_division,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1544, in precision_recall_fscore_support\n",
            "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1367, in _check_set_wise_labels\n",
            "    \"choose another average setting, one of %r.\" % (y_type, average_options)\n",
            "ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5] END ....................n_estimators=200;, score=nan total time=  17.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 221, in __call__\n",
            "    sample_weight=sample_weight,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 264, in _score\n",
            "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1131, in f1_score\n",
            "    zero_division=zero_division,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1270, in fbeta_score\n",
            "    zero_division=zero_division,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1544, in precision_recall_fscore_support\n",
            "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\", line 1367, in _check_set_wise_labels\n",
            "    \"choose another average setting, one of %r.\" % (y_type, average_options)\n",
            "ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5] END ....................n_estimators=200;, score=nan total time=  17.3s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-505da91d18b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mGSCV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreg_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"f1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mGSCV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m                     for (cand_idx, parameters), (split_idx, (train, test)) in product(\n\u001b[0;32m--> 851\u001b[0;31m                         \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m                     )\n\u001b[1;32m    853\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    465\u001b[0m                     \u001b[0mn_samples_bootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_samples_bootstrap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                 )\n\u001b[0;32m--> 467\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m             )\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    940\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m         )\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    418\u001b[0m             )\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "reg_params = {\n",
        "\"n_estimators\": [100,200,50]\n",
        "\n",
        "},\n",
        "\n",
        "GSCV = GridSearchCV(rfc,reg_params,scoring=\"f1\",verbose =3)\n",
        "GSCV.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--uNsN8BtGsV"
      },
      "source": [
        "## Neural Networks using SKLearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NASf1yd-Gqkd"
      },
      "source": [
        "###Base Case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UdtSIWn9s9WU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e80d9deb-b584-4a83-8115-76685522a6c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8, 8, 8, ..., 8, 8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "NN = MLPClassifier(hidden_layer_sizes=(150,60), solver='adam', random_state=1, activation =\"relu\", learning_rate=\"adaptive\")\n",
        "\n",
        "NN.fit(X_train,y_train)\n",
        "pred = NN.predict(X_test)\n",
        "pred "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Guh-WaHps9a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b5b17b0-014b-4760-a3ac-85ffdd044679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           3       0.00      0.00      0.00      1104\n",
            "           4       0.00      0.00      0.00       107\n",
            "           5       0.00      0.00      0.00      1453\n",
            "           6       0.00      0.00      0.00       376\n",
            "           7       0.00      0.00      0.00      1687\n",
            "           8       0.13      1.00      0.23      3639\n",
            "           9       0.00      0.00      0.00      2902\n",
            "          12       0.00      0.00      0.00        92\n",
            "          15       0.00      0.00      0.00       315\n",
            "          18       0.00      0.00      0.00       175\n",
            "          19       0.00      0.00      0.00       109\n",
            "          20       0.00      0.00      0.00       198\n",
            "          21       0.00      0.00      0.00       185\n",
            "          22       0.00      0.00      0.00       277\n",
            "          23       0.00      0.00      0.00        43\n",
            "          24       0.00      0.00      0.00       758\n",
            "          25       0.00      0.00      0.00      1117\n",
            "          26       0.00      0.00      0.00       155\n",
            "          27       0.00      0.00      0.00       235\n",
            "          28       0.00      0.00      0.00       138\n",
            "          29       0.00      0.00      0.00       134\n",
            "          30       0.00      0.00      0.00       347\n",
            "          31       0.00      0.00      0.00       178\n",
            "          32       0.00      0.00      0.00       584\n",
            "          33       0.00      0.00      0.00       384\n",
            "          34       0.00      0.00      0.00       237\n",
            "          35       0.00      0.00      0.00       623\n",
            "          36       0.00      0.00      0.00       939\n",
            "          37       0.00      0.00      0.00       836\n",
            "          38       0.00      0.00      0.00       859\n",
            "          39       0.00      0.00      0.00      2940\n",
            "          40       0.60      0.00      0.00      1829\n",
            "          41       0.00      0.00      0.00       162\n",
            "          42       0.00      0.00      0.00       528\n",
            "          43       0.00      0.00      0.00       282\n",
            "          44       0.00      0.00      0.00       365\n",
            "         999       0.00      0.00      0.00      2363\n",
            "\n",
            "    accuracy                           0.13     28655\n",
            "   macro avg       0.02      0.03      0.01     28655\n",
            "weighted avg       0.05      0.13      0.03     28655\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn. metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXKW7B-Ns9dL"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "print(\"Overall Accuracy:\",accuracy_score(y_test, pred))\n",
        "print(\"Overall Precision:\",precision_score(y_test, pred, average='macro'))\n",
        "print(\"Overall Recall:\",recall_score(y_test, pred, average='macro'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zzWsPDjs9hD"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix = confusion_matrix(y_test, pred)\n",
        "print(confusion_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrV7_zjMGwlI"
      },
      "source": [
        "###Hyperparameter Tuning "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GucOpzxs9lW"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "NN_params = {\n",
        "    \n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjJ0PxoHvlkY"
      },
      "source": [
        "#Model Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWLc1e8RtS3C"
      },
      "source": [
        "## Comparing models with cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnYub0M7s9oB"
      },
      "outputs": [],
      "source": [
        "# Import required libraries for performance metrics\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "# Import required libraries for machine learning classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\n",
        "# Define dictionary with performance metrics we want to use\n",
        "score = {'accuracy':make_scorer(accuracy_score), \n",
        "           'precision':make_scorer(precision_score),\n",
        "           'recall':make_scorer(recall_score), \n",
        "           'f1_score':make_scorer(f1_score)}\n",
        "\n",
        "# Initializing the machine learning classifiers\n",
        "log_model = LogisticRegression(C=1/0.1, solver='lbfgs', multi_class='auto', max_iter=10000)\n",
        "rfc_model = RandomForestClassifier()\n",
        "nn_model = MLPClassifier(solver='lbfgs', alpha=1e-5, random_state=1)\n",
        "\n",
        "def model_evaluation(X, y, kfolds):\n",
        "    \n",
        "    \n",
        "    # Performing cross-validation to each machine learning classifier\n",
        "    \n",
        "    rfc = cross_validate(rfc_model, X, y, cv=kfolds, scoring=score)\n",
        "    nn = cross_validate(nn_model, X, y, cv=kfolds, scoring=score)\n",
        "\n",
        "    # Create a data frame with the models performance metrics scores\n",
        "    models_scores_frame = pd.DataFrame({'Logistic Regression':[log['test_accuracy'].mean(),\n",
        "                                                               log['test_precision'].mean(),\n",
        "                                                               log['test_recall'].mean(),\n",
        "                                                               log['test_f1_score'].mean()],\n",
        "                                       \n",
        "                                      'Random Forests':[rfc['test_accuracy'].mean(),\n",
        "                                                       rfc['test_precision'].mean(),\n",
        "                                                       rfc['test_recall'].mean(),\n",
        "                                                       rfc['test_f1_score'].mean()],\n",
        "                                       \n",
        "                                      'Neural Networks':[nn['test_accuracy'].mean(),\n",
        "                                                              nn['test_precision'].mean(),\n",
        "                                                              nn['test_recall'].mean(),\n",
        "                                                              nn['test_f1_score'].mean()]},\n",
        "                                      \n",
        "                                      index=['Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
        "    \n",
        "      # Add 'Best Score' column\n",
        "    models_scores_frame['Highest Score'] = models_scores_frame.idxmax(axis=1)\n",
        "    \n",
        "    # Return models performance metrics scores data frame\n",
        "    return(models_scores_frame)\n",
        "  \n",
        "# Run models_evaluation function\n",
        "model_evaluation(X, y, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvbfQwYauxRz"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def baseline_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(50, input_dim=5, activation='relu'))\n",
        "\tmodel.add(Dense(3, activation='softmax'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "NN= KerasClassifier(build_fn=baseline_model, epochs =200,batch_size=5,verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = cross_val_score(, X, dummy_y, cv=kfold)\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
        "kfold = KFold(n_splits=10, shuffle=True)\n",
        "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "metadata": {
        "id": "VtWkMO-Q3T1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u0dK9BUdYWY"
      },
      "source": [
        "# Ethics & Privacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp52ycowdYWZ"
      },
      "source": [
        "Our data does not involve anyones data or identity so it would be difficult to find a breach of ethics or privacy. One ethical dilemma that might arise is the increasing amount of data available to big corporations and how they’re using this big data to fine tune their products to keep the average person consuming even more. It would be good to question whether it is completely ethical for corporations to treat everyone as a number to maximize their profits.\n",
        "\n",
        "Another note on privacy is the trip type classification labels. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhplYiLCdYWZ"
      },
      "source": [
        "# Team Expectations "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dqZncRZdYWa"
      },
      "source": [
        "Put things here that cement how you will interact/communicate as a team, how you will handle conflict and difficulty, how you will handle making decisions and setting goals/schedule, how much work you expect from each other, how you will handle deadlines, etc...\n",
        "\n",
        "- Arrange bi-weekly meetings that works with everyones schedule\n",
        "- Use a discord server to communicate with one another\n",
        "- Make use of project managment software to track progress\n",
        "- Be mindful of git -pull-push-overwrites such that no code is overwritten or needlessly repeated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ndy_9D4UdYWa"
      },
      "source": [
        "# Project Timeline Proposal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYsKpfridYWb"
      },
      "source": [
        "UPDATE THE PROPOSAL TIMELINE ACCORDING TO WHAT HAS ACTUALLY HAPPENED AND HOW IT HAS EFFECTED YOUR FUTURE PLANS\n",
        "\n",
        "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
        "|---|---|---|---|\n",
        "| 4/24  |  3:30 PM |  Edit, finalize, and submit proposal; Search for datasets  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part | \n",
        "| 5/16  |  9 PM |  Delegate tasks for first checkpoint and discuss wrangling, cleaning, and EDA plan | Import and wrangle data, do some EDA | \n",
        "| 5/19  | 9 PM  | Edit and finalize data cleaning and wrangling/EDA  | Review/discuss EDA, debug, and submit checkpoint   |\n",
        "| 5/23  | 7 PM  | Finalize project/conclusion/discussion | Discuss conclusion   |\n",
        "| 6/8  | Before 11:59 PM  | NA | Turn in Final Project  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ0o-4_PdYWc"
      },
      "source": [
        "# Footnotes\n",
        "<a name=\"first\"></a>1.[^](#firstnote): Oliver Kramer. K Nearest Neighbors. https://link.springer.com/chapter/10.1007/978-3-642-38652-7_2<br> \n",
        "<a name=\"second\"></a>2.[^](#secondnote): Srivastava et al. (1999) Parallel Forumlations of Decision Tree Classfication Algorithms. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.41.7475&rep=rep1&type=pdf<br>\n",
        "<a name=\"third\"></a>3.[^](#thirdnote): M. Pal (2005).Random forest classifier for remote sensing classification. https://www.tandfonline.com/doi/pdf/10.1080/01431160412331269698?casa_token=e78vG4sBDLcAAAAA:p9nt0mSjEMuazyQsDjprmwIIFt9aNRk9EtF7eKRyNozF6FsAskuvXKrMxnnftOK0xFjlUm5MX9g.<br>\n",
        "<a name=\"fourth\"></a>4.[^](#fourthnote): Stephan Dreiseitl and Lucila Ohno_Machado. (2002). Logistic regression and artificial neural network classification models: a methodology review. https://www.sciencedirect.com/science/article/pii/S1532046403000340.<br>\n",
        "<a name=\"fifth\"></a>5.[^](#fifthnote): Cui et al. (2018). Deep Embedding Logistic Regression. https://ieeexplore.ieee.org/document/8588790\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1rVn2mEdYWd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Final Project_group004.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
